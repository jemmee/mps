{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d945e70-151b-4603-b7d2-b04d4438cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Install the full training bundle (Recommended)\n",
    "!pip install \"mlx-lm[train]\"\n",
    "\n",
    "# Option 2: Just install the missing library\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6b4a9-a3d9-4128-b568-8b1136ef0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets library found!\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import datasets; print('Datasets library found!')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af2516e-f7ff-4846-8080-49d161d3b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 16027.62it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.014% (1.769M/12772.913M)\n",
      "Starting training..., iters: 500\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:05<00:00,  2.61s/it]\n",
      "Iter 1: Val loss 9.129, Val took 5.236s\n",
      "Iter 10: Train loss 6.189, Learning Rate 1.000e-05, It/sec 0.283, Tokens/sec 21.838, Trained Tokens 772, Peak mem 8.199 GB\n",
      "Iter 20: Train loss 4.356, Learning Rate 1.000e-05, It/sec 0.293, Tokens/sec 22.588, Trained Tokens 1544, Peak mem 8.199 GB\n",
      "Iter 30: Train loss 3.503, Learning Rate 1.000e-05, It/sec 0.283, Tokens/sec 21.809, Trained Tokens 2316, Peak mem 8.199 GB\n",
      "Iter 40: Train loss 2.887, Learning Rate 1.000e-05, It/sec 0.286, Tokens/sec 22.117, Trained Tokens 3088, Peak mem 8.199 GB\n",
      "Iter 50: Train loss 2.540, Learning Rate 1.000e-05, It/sec 0.282, Tokens/sec 21.756, Trained Tokens 3860, Peak mem 8.199 GB\n",
      "Iter 60: Train loss 2.157, Learning Rate 1.000e-05, It/sec 0.282, Tokens/sec 21.782, Trained Tokens 4632, Peak mem 8.199 GB\n",
      "Iter 70: Train loss 1.795, Learning Rate 1.000e-05, It/sec 0.278, Tokens/sec 21.437, Trained Tokens 5404, Peak mem 8.199 GB\n",
      "Iter 80: Train loss 1.466, Learning Rate 1.000e-05, It/sec 0.279, Tokens/sec 21.543, Trained Tokens 6176, Peak mem 8.199 GB\n",
      "Iter 90: Train loss 1.132, Learning Rate 1.000e-05, It/sec 0.270, Tokens/sec 20.859, Trained Tokens 6948, Peak mem 8.199 GB\n",
      "Iter 100: Train loss 0.811, Learning Rate 1.000e-05, It/sec 0.268, Tokens/sec 20.658, Trained Tokens 7720, Peak mem 8.199 GB\n",
      "Iter 100: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.266, Tokens/sec 20.506, Trained Tokens 8492, Peak mem 8.206 GB\n",
      "Iter 120: Train loss 0.363, Learning Rate 1.000e-05, It/sec 0.265, Tokens/sec 20.432, Trained Tokens 9264, Peak mem 8.206 GB\n",
      "Iter 130: Train loss 0.222, Learning Rate 1.000e-05, It/sec 0.257, Tokens/sec 19.875, Trained Tokens 10036, Peak mem 8.206 GB\n",
      "Iter 140: Train loss 0.139, Learning Rate 1.000e-05, It/sec 0.267, Tokens/sec 20.612, Trained Tokens 10808, Peak mem 8.206 GB\n",
      "Iter 150: Train loss 0.093, Learning Rate 1.000e-05, It/sec 0.265, Tokens/sec 20.442, Trained Tokens 11580, Peak mem 8.206 GB\n",
      "Iter 160: Train loss 0.077, Learning Rate 1.000e-05, It/sec 0.271, Tokens/sec 20.924, Trained Tokens 12352, Peak mem 8.206 GB\n",
      "Iter 170: Train loss 0.068, Learning Rate 1.000e-05, It/sec 0.269, Tokens/sec 20.797, Trained Tokens 13124, Peak mem 8.206 GB\n",
      "Iter 180: Train loss 0.061, Learning Rate 1.000e-05, It/sec 0.269, Tokens/sec 20.740, Trained Tokens 13896, Peak mem 8.206 GB\n",
      "Iter 190: Train loss 0.055, Learning Rate 1.000e-05, It/sec 0.258, Tokens/sec 19.953, Trained Tokens 14668, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:04<00:00,  2.24s/it]\n",
      "Iter 200: Val loss 0.808, Val took 4.530s\n",
      "Iter 200: Train loss 0.056, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 19.320, Trained Tokens 15440, Peak mem 8.206 GB\n",
      "Iter 200: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.056, Learning Rate 1.000e-05, It/sec 0.257, Tokens/sec 19.836, Trained Tokens 16212, Peak mem 8.206 GB\n",
      "Iter 220: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.234, Tokens/sec 18.065, Trained Tokens 16984, Peak mem 8.206 GB\n",
      "Iter 230: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.247, Tokens/sec 19.043, Trained Tokens 17756, Peak mem 8.206 GB\n",
      "Iter 240: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 19.633, Trained Tokens 18528, Peak mem 8.206 GB\n",
      "Iter 250: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.256, Tokens/sec 19.767, Trained Tokens 19300, Peak mem 8.206 GB\n",
      "Iter 260: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.255, Tokens/sec 19.651, Trained Tokens 20072, Peak mem 8.206 GB\n",
      "Iter 270: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.251, Tokens/sec 19.369, Trained Tokens 20844, Peak mem 8.206 GB\n",
      "Iter 280: Train loss 0.055, Learning Rate 1.000e-05, It/sec 0.261, Tokens/sec 20.157, Trained Tokens 21616, Peak mem 8.206 GB\n",
      "Iter 290: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.260, Tokens/sec 20.034, Trained Tokens 22388, Peak mem 8.206 GB\n",
      "Iter 300: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.258, Tokens/sec 19.923, Trained Tokens 23160, Peak mem 8.206 GB\n",
      "Iter 300: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.260, Tokens/sec 20.106, Trained Tokens 23932, Peak mem 8.206 GB\n",
      "Iter 320: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.260, Tokens/sec 20.080, Trained Tokens 24704, Peak mem 8.206 GB\n",
      "Iter 330: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.258, Tokens/sec 19.950, Trained Tokens 25476, Peak mem 8.206 GB\n",
      "Iter 340: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.259, Tokens/sec 20.009, Trained Tokens 26248, Peak mem 8.206 GB\n",
      "Iter 350: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.259, Tokens/sec 20.018, Trained Tokens 27020, Peak mem 8.206 GB\n",
      "Iter 360: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.255, Tokens/sec 19.659, Trained Tokens 27792, Peak mem 8.206 GB\n",
      "Iter 370: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.248, Tokens/sec 19.118, Trained Tokens 28564, Peak mem 8.206 GB\n",
      "Iter 380: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.242, Tokens/sec 18.717, Trained Tokens 29336, Peak mem 8.206 GB\n",
      "Iter 390: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.253, Tokens/sec 19.543, Trained Tokens 30108, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:05<00:00,  2.74s/it]\n",
      "Iter 400: Val loss 0.810, Val took 5.516s\n",
      "Iter 400: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.227, Tokens/sec 17.505, Trained Tokens 30880, Peak mem 8.206 GB\n",
      "Iter 400: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.262, Tokens/sec 20.207, Trained Tokens 31652, Peak mem 8.206 GB\n",
      "Iter 420: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.241, Tokens/sec 18.594, Trained Tokens 32424, Peak mem 8.206 GB\n",
      "Iter 430: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.257, Tokens/sec 19.869, Trained Tokens 33196, Peak mem 8.206 GB\n",
      "Iter 440: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.261, Tokens/sec 20.130, Trained Tokens 33968, Peak mem 8.206 GB\n",
      "Iter 450: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.240, Tokens/sec 18.523, Trained Tokens 34740, Peak mem 8.206 GB\n",
      "Iter 460: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 19.639, Trained Tokens 35512, Peak mem 8.206 GB\n",
      "Iter 470: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.237, Tokens/sec 18.301, Trained Tokens 36284, Peak mem 8.206 GB\n",
      "Iter 480: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.235, Tokens/sec 18.169, Trained Tokens 37056, Peak mem 8.206 GB\n",
      "Iter 490: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.240, Tokens/sec 18.496, Trained Tokens 37828, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:05<00:00,  2.65s/it]\n",
      "Iter 500: Val loss 0.787, Val took 5.386s\n",
      "Iter 500: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.239, Tokens/sec 18.481, Trained Tokens 38600, Peak mem 8.206 GB\n",
      "Iter 500: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000500_adapters.safetensors.\n",
      "Saved final weights to anime_adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora \\\n",
    "  --model mlx-community/gemma-3-12b-it-4bit \\\n",
    "  --train \\\n",
    "  --data ./anime_data \\\n",
    "  --iters 500 \\\n",
    "  --batch-size 1 \\\n",
    "  --learning-rate 1e-5 \\\n",
    "  --adapter-path ./anime_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27aa03-1e1e-4bbc-9cbc-d1f2a158c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9727a77dca6482a884920bdf8fabace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Who are the main characters of GXYZ123?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========e-tuned): \n",
      "The primary characters include 'Protocol-X,' a sentient glitch trying to restore its memory, and 'User-774,' an anonymous moderator who discovers a hidden world within the site's code.\n",
      "==========\n",
      "Prompt: 21 tokens, 6.835 tokens-per-sec\n",
      "Generation: 42 tokens, 7.597 tokens-per-sec\n",
      "Peak memory: 7.328 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# 1. Load the model (the 4-bit version is essential for M1 RAM efficiency)\n",
    "model_name = \"mlx-community/gemma-3-12b-it-4bit\"\n",
    "model, tokenizer = load(model_name, adapter_path=\"./anime_adapters\")\n",
    "\n",
    "# 2. Initialize an empty conversation history\n",
    "messages = []\n",
    "\n",
    "def chat_with_memory():\n",
    "    print(\"--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\")\n",
    "    \n",
    "    while True:\n",
    "        # Who are the main characters of GXYZ123?\n",
    "        # What's Rudo's vital instrument in anime Gachiakuta?\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        if user_input.lower() == 'clear':\n",
    "            messages.clear()\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Format history into the Gemma-specific string format\n",
    "        # This adds the <start_of_turn> and <end_of_turn> tags automatically\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"\\nGemma (Fine-tuned): \", end=\"\", flush=True)\n",
    "        response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=prompt, \n",
    "            max_tokens=1000, \n",
    "            verbose=True, # Streaming enabled\n",
    "        )\n",
    "        \n",
    "        # Save the model's response to history for next time\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Run the chat\n",
    "chat_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a194081-1d9f-4e07-9fb0-1d2e98d857da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (MLX M1)",
   "language": "python",
   "name": "mlx_chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
