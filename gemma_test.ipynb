{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6162c6-5743-489e-8117-51463b122f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a conda environment (or venv)\n",
    "!conda create -n mlx_chat python=3.11 -y\n",
    "!conda activate mlx_chat\n",
    "\n",
    "# 2. Install MLX and Jupyter kernel tools\n",
    "!pip install mlx-lm ipykernel\n",
    "\n",
    "# 3. Add this environment as a Jupyter Kernel\n",
    "!python -m ipykernel install --user --name mlx_chat --display-name \"Python 3.11 (MLX M1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703e6a20-a5a8-4f6f-823c-8facc3d4e804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Default Device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "# Check if the Metal (GPU) backend is available\n",
    "gpu_available = mx.metal.is_available()\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "\n",
    "# MLX uses the GPU by default. You can see the default device here:\n",
    "print(f\"Default Device: {mx.default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf4d60-e01b-4e69-9683-02f4b839a85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46076de26494602b568a8fb6d9e4672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gemma Chatbot (Type 'clear' to reset or 'exit' to stop) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  How many states are there in the United States?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "There are **50** states in the United States. \n",
      "\n",
      "==========\n",
      "Prompt: 19 tokens, 18.078 tokens-per-sec\n",
      "Generation: 15 tokens, 35.381 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Print all prefectures of Japan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "You're asking for a very long list!  There are **7 prefectures** in Japan.  \n",
      "\n",
      "Here they are, in order from north to south:\n",
      "\n",
      "1. **Hokkaido**\n",
      "2. **Aomori**\n",
      "3. **Iwate**\n",
      "4. **Miyagi**\n",
      "5. **Akita**\n",
      "6. **Yamagata**\n",
      "7. **Fukushima**\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more about any of these prefectures!  \n",
      "\n",
      "==========\n",
      "Prompt: 48 tokens, 168.365 tokens-per-sec\n",
      "Generation: 108 tokens, 33.639 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Explain the world-building of the anime Akira\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Akira's world-building is a complex and fascinating tapestry, blending real-world elements with a unique, dystopian future.  Here's a breakdown of some key aspects:\n",
      "\n",
      "**Setting:**\n",
      "\n",
      "* **Neo-Tokyo:**  The story takes place in a futuristic, technologically advanced city that serves as a stark contrast to the original Tokyo.  It's a city of towering skyscrapers, advanced technology, and a complex social structure.\n",
      "* **Post-Nuclear War:** The world is scarred by a devastating nuclear war, leaving behind a legacy of environmental destruction, social upheaval, and a sense of perpetual danger.\n",
      "* **Political Corruption:** The government is corrupt and power-hungry, with a focus on control and suppression. This leads to a complex power dynamic and a sense of distrust among citizens.\n",
      "* **Social Divide:** The city is divided into distinct social classes, with the wealthy elite living in luxury while the poor struggle to survive in overcrowded slums. This creates a stark contrast between the haves and have-nots.\n",
      "\n",
      "**Themes:**\n",
      "\n",
      "* **Power and Corruption:** The story explores the corrupting influence of power, how it can be used for personal gain, and the consequences of unchecked ambition.\n",
      "* **Social Inequality:** The stark social divide in Neo-Tokyo highlights the dangers of unchecked inequality and the potential for social unrest.\n",
      "* **The Nature of Humanity:** Akira explores the complexities of human nature, the potential for both good and evil, and the struggle between individual desires and societal pressures.\n",
      "* **Technology and its Impact:** The story explores the potential benefits and dangers of advanced technology, how it can be used for both good and evil, and the impact it has on society.\n",
      "* **Loss, Trauma, and Identity:** The characters in Akira grapple with loss, trauma, and the search for identity, themes that resonate with audiences on a personal level.\n",
      "\n",
      "**Visuals and Symbolism:**\n",
      "\n",
      "* **Visual Style:** The anime's distinctive visual style, with its bold colors, dynamic action sequences, and detailed character designs, contributes to the world's overall atmosphere.\n",
      "* **Symbolism:** The story is rich in symbolism, with elements like the \"Akira\" motorcycle, the \"Tetsuo\" character, and the \"ghost\" imagery representing themes of power, corruption, and the human condition.\n",
      "\n",
      "\n",
      "**Overall, Akira's world-building is a powerful and thought-provoking exploration of the human condition, the consequences of war, and the potential for both good and evil in a technologically advanced future.** \n",
      "\n",
      "\n",
      "Let me know if you'd like to delve deeper into any specific aspect of Akira's world-building! \n",
      "\n",
      "==========\n",
      "Prompt: 172 tokens, 247.346 tokens-per-sec\n",
      "Generation: 546 tokens, 31.804 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  What's Rudo's vital instrument in anime Gachiakuta?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "Rudo's vital instrument in the anime Gachiakuta is the **\"Rudo-Rudo\"**.  \n",
      "\n",
      "It's a unique, powerful, and somewhat mysterious instrument that plays a crucial role in his abilities and the overall story.  \n",
      "\n",
      "Let me know if you'd like to know more about the \"Rudo-Rudo\" or other aspects of Gachiakuta!  \n",
      "\n",
      "==========\n",
      "Prompt: 741 tokens, 274.089 tokens-per-sec\n",
      "Generation: 85 tokens, 32.434 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Are you sure? I don't think it's Rudo-Rudo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "You are absolutely right!  My apologies, I seem to have gotten my wires crossed.  \n",
      "\n",
      "Rudo's vital instrument in Gachiakuta is actually the **\"Rudo\"**  (not Rudo-Rudo).  \n",
      "\n",
      "I am still under development and learning to process information accurately.  Thank you for pointing out my mistake!  \n",
      "\n",
      "\n",
      "Do you have any other questions about Gachiakuta?  I'll do my best to answer them correctly.  ðŸ˜Š  \n",
      "\n",
      "==========\n",
      "Prompt: 850 tokens, 277.055 tokens-per-sec\n",
      "Generation: 103 tokens, 32.072 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# 1. Load the model (the 4-bit version is essential for M1 RAM efficiency)\n",
    "model_name = \"mlx-community/gemma-2-2b-it-4bit\"\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "# 2. Initialize an empty conversation history\n",
    "messages = []\n",
    "\n",
    "def chat_with_memory():\n",
    "    print(\"--- Gemma Chatbot (Type 'clear' to reset or 'exit' to stop) ---\")\n",
    "    \n",
    "    while True:\n",
    "        # How many states are there in the United States?\n",
    "        # Print all prefectures of Japan\n",
    "        # Explain the world-building of the anime Akira\n",
    "        # What's Rudo's vital instrument in anime Gachiakuta?\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        if user_input.lower() == 'clear':\n",
    "            messages.clear()\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Format history into the Gemma-specific string format\n",
    "        # This adds the <start_of_turn> and <end_of_turn> tags automatically\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"\\nGemma: \", end=\"\", flush=True)\n",
    "        response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=prompt, \n",
    "            max_tokens=1000, \n",
    "            verbose=True # Streaming enabled\n",
    "        )\n",
    "        \n",
    "        # Save the model's response to history for next time\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Run the chat\n",
    "chat_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8b609-922b-4f7c-932a-83868ec31302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (MLX M1)",
   "language": "python",
   "name": "mlx_chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
