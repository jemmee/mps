{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d945e70-151b-4603-b7d2-b04d4438cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Install the full training bundle (Recommended)\n",
    "!pip install \"mlx-lm[train]\"\n",
    "\n",
    "# Option 2: Just install the missing library\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6b4a9-a3d9-4128-b568-8b1136ef0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets library found!\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import datasets; print('Datasets library found!')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af2516e-f7ff-4846-8080-49d161d3b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Fetching 7 files: 100%|███████████████████████| 7/7 [00:00<00:00, 112923.57it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.038% (0.983M/2614.342M)\n",
      "Starting training..., iters: 500\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Iter 1: Val loss 6.137, Val took 1.754s\n",
      "Iter 10: Train loss 4.610, Learning Rate 1.000e-05, It/sec 1.076, Tokens/sec 82.758, Trained Tokens 769, Peak mem 2.301 GB\n",
      "Iter 20: Train loss 3.438, Learning Rate 1.000e-05, It/sec 1.444, Tokens/sec 111.037, Trained Tokens 1538, Peak mem 2.301 GB\n",
      "Iter 30: Train loss 2.651, Learning Rate 1.000e-05, It/sec 1.427, Tokens/sec 109.771, Trained Tokens 2307, Peak mem 2.301 GB\n",
      "Iter 40: Train loss 2.060, Learning Rate 1.000e-05, It/sec 1.428, Tokens/sec 109.829, Trained Tokens 3076, Peak mem 2.301 GB\n",
      "Iter 50: Train loss 1.514, Learning Rate 1.000e-05, It/sec 1.419, Tokens/sec 109.111, Trained Tokens 3845, Peak mem 2.301 GB\n",
      "Iter 60: Train loss 1.109, Learning Rate 1.000e-05, It/sec 1.247, Tokens/sec 95.864, Trained Tokens 4614, Peak mem 2.301 GB\n",
      "Iter 70: Train loss 0.730, Learning Rate 1.000e-05, It/sec 1.281, Tokens/sec 98.517, Trained Tokens 5383, Peak mem 2.301 GB\n",
      "Iter 80: Train loss 0.470, Learning Rate 1.000e-05, It/sec 1.270, Tokens/sec 97.670, Trained Tokens 6152, Peak mem 2.301 GB\n",
      "Iter 90: Train loss 0.339, Learning Rate 1.000e-05, It/sec 1.234, Tokens/sec 94.883, Trained Tokens 6921, Peak mem 2.301 GB\n",
      "Iter 100: Train loss 0.271, Learning Rate 1.000e-05, It/sec 1.209, Tokens/sec 93.007, Trained Tokens 7690, Peak mem 2.301 GB\n",
      "Iter 100: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.237, Learning Rate 1.000e-05, It/sec 1.180, Tokens/sec 90.704, Trained Tokens 8459, Peak mem 2.305 GB\n",
      "Iter 120: Train loss 0.216, Learning Rate 1.000e-05, It/sec 1.179, Tokens/sec 90.676, Trained Tokens 9228, Peak mem 2.305 GB\n",
      "Iter 130: Train loss 0.198, Learning Rate 1.000e-05, It/sec 1.160, Tokens/sec 89.186, Trained Tokens 9997, Peak mem 2.306 GB\n",
      "Iter 140: Train loss 0.187, Learning Rate 1.000e-05, It/sec 1.190, Tokens/sec 91.497, Trained Tokens 10766, Peak mem 2.306 GB\n",
      "Iter 150: Train loss 0.179, Learning Rate 1.000e-05, It/sec 1.068, Tokens/sec 82.114, Trained Tokens 11535, Peak mem 2.306 GB\n",
      "Iter 160: Train loss 0.171, Learning Rate 1.000e-05, It/sec 1.192, Tokens/sec 91.665, Trained Tokens 12304, Peak mem 2.306 GB\n",
      "Iter 170: Train loss 0.167, Learning Rate 1.000e-05, It/sec 1.192, Tokens/sec 91.666, Trained Tokens 13073, Peak mem 2.306 GB\n",
      "Iter 180: Train loss 0.158, Learning Rate 1.000e-05, It/sec 1.197, Tokens/sec 92.071, Trained Tokens 13842, Peak mem 2.306 GB\n",
      "Iter 190: Train loss 0.144, Learning Rate 1.000e-05, It/sec 1.180, Tokens/sec 90.719, Trained Tokens 14611, Peak mem 2.306 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:00<00:00,  3.66it/s]\n",
      "Iter 200: Val loss 0.567, Val took 0.554s\n",
      "Iter 200: Train loss 0.143, Learning Rate 1.000e-05, It/sec 1.186, Tokens/sec 91.177, Trained Tokens 15380, Peak mem 2.306 GB\n",
      "Iter 200: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.137, Learning Rate 1.000e-05, It/sec 1.202, Tokens/sec 92.439, Trained Tokens 16149, Peak mem 2.306 GB\n",
      "Iter 220: Train loss 0.131, Learning Rate 1.000e-05, It/sec 1.093, Tokens/sec 84.079, Trained Tokens 16918, Peak mem 2.306 GB\n",
      "Iter 230: Train loss 0.126, Learning Rate 1.000e-05, It/sec 1.180, Tokens/sec 90.717, Trained Tokens 17687, Peak mem 2.306 GB\n",
      "Iter 240: Train loss 0.125, Learning Rate 1.000e-05, It/sec 1.147, Tokens/sec 88.173, Trained Tokens 18456, Peak mem 2.306 GB\n",
      "Iter 250: Train loss 0.118, Learning Rate 1.000e-05, It/sec 1.163, Tokens/sec 89.417, Trained Tokens 19225, Peak mem 2.306 GB\n",
      "Iter 260: Train loss 0.114, Learning Rate 1.000e-05, It/sec 1.166, Tokens/sec 89.675, Trained Tokens 19994, Peak mem 2.306 GB\n",
      "Iter 270: Train loss 0.111, Learning Rate 1.000e-05, It/sec 1.162, Tokens/sec 89.348, Trained Tokens 20763, Peak mem 2.306 GB\n",
      "Iter 280: Train loss 0.106, Learning Rate 1.000e-05, It/sec 1.163, Tokens/sec 89.399, Trained Tokens 21532, Peak mem 2.306 GB\n",
      "Iter 290: Train loss 0.103, Learning Rate 1.000e-05, It/sec 1.119, Tokens/sec 86.052, Trained Tokens 22301, Peak mem 2.306 GB\n",
      "Iter 300: Train loss 0.100, Learning Rate 1.000e-05, It/sec 1.123, Tokens/sec 86.335, Trained Tokens 23070, Peak mem 2.306 GB\n",
      "Iter 300: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.098, Learning Rate 1.000e-05, It/sec 1.115, Tokens/sec 85.750, Trained Tokens 23839, Peak mem 2.306 GB\n",
      "Iter 320: Train loss 0.093, Learning Rate 1.000e-05, It/sec 1.059, Tokens/sec 81.404, Trained Tokens 24608, Peak mem 2.306 GB\n",
      "Iter 330: Train loss 0.092, Learning Rate 1.000e-05, It/sec 1.017, Tokens/sec 78.212, Trained Tokens 25377, Peak mem 2.306 GB\n",
      "Iter 340: Train loss 0.089, Learning Rate 1.000e-05, It/sec 0.970, Tokens/sec 74.594, Trained Tokens 26146, Peak mem 2.306 GB\n",
      "Iter 350: Train loss 0.087, Learning Rate 1.000e-05, It/sec 0.919, Tokens/sec 70.670, Trained Tokens 26915, Peak mem 2.306 GB\n",
      "Iter 360: Train loss 0.084, Learning Rate 1.000e-05, It/sec 0.987, Tokens/sec 75.904, Trained Tokens 27684, Peak mem 2.306 GB\n",
      "Iter 370: Train loss 0.083, Learning Rate 1.000e-05, It/sec 1.009, Tokens/sec 77.571, Trained Tokens 28453, Peak mem 2.306 GB\n",
      "Iter 380: Train loss 0.079, Learning Rate 1.000e-05, It/sec 1.014, Tokens/sec 77.945, Trained Tokens 29222, Peak mem 2.306 GB\n",
      "Iter 390: Train loss 0.076, Learning Rate 1.000e-05, It/sec 0.992, Tokens/sec 76.291, Trained Tokens 29991, Peak mem 2.306 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:00<00:00,  2.59it/s]\n",
      "Iter 400: Val loss 0.561, Val took 0.777s\n",
      "Iter 400: Train loss 0.075, Learning Rate 1.000e-05, It/sec 0.954, Tokens/sec 73.339, Trained Tokens 30760, Peak mem 2.306 GB\n",
      "Iter 400: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.074, Learning Rate 1.000e-05, It/sec 0.877, Tokens/sec 67.423, Trained Tokens 31529, Peak mem 2.306 GB\n",
      "Iter 420: Train loss 0.073, Learning Rate 1.000e-05, It/sec 0.899, Tokens/sec 69.155, Trained Tokens 32298, Peak mem 2.306 GB\n",
      "Iter 430: Train loss 0.069, Learning Rate 1.000e-05, It/sec 0.869, Tokens/sec 66.840, Trained Tokens 33067, Peak mem 2.306 GB\n",
      "Iter 440: Train loss 0.068, Learning Rate 1.000e-05, It/sec 0.872, Tokens/sec 67.072, Trained Tokens 33836, Peak mem 2.306 GB\n",
      "Iter 450: Train loss 0.067, Learning Rate 1.000e-05, It/sec 0.887, Tokens/sec 68.232, Trained Tokens 34605, Peak mem 2.306 GB\n",
      "Iter 460: Train loss 0.064, Learning Rate 1.000e-05, It/sec 0.914, Tokens/sec 70.252, Trained Tokens 35374, Peak mem 2.306 GB\n",
      "Iter 470: Train loss 0.065, Learning Rate 1.000e-05, It/sec 0.903, Tokens/sec 69.436, Trained Tokens 36143, Peak mem 2.306 GB\n",
      "Iter 480: Train loss 0.065, Learning Rate 1.000e-05, It/sec 0.900, Tokens/sec 69.183, Trained Tokens 36912, Peak mem 2.306 GB\n",
      "Iter 490: Train loss 0.062, Learning Rate 1.000e-05, It/sec 0.906, Tokens/sec 69.676, Trained Tokens 37681, Peak mem 2.306 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:00<00:00,  2.65it/s]\n",
      "Iter 500: Val loss 0.548, Val took 0.759s\n",
      "Iter 500: Train loss 0.061, Learning Rate 1.000e-05, It/sec 0.848, Tokens/sec 65.211, Trained Tokens 38450, Peak mem 2.306 GB\n",
      "Iter 500: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000500_adapters.safetensors.\n",
      "Saved final weights to anime_adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora \\\n",
    "  --model mlx-community/gemma-2-2b-it-4bit \\\n",
    "  --train \\\n",
    "  --data ./anime_data \\\n",
    "  --iters 500 \\\n",
    "  --batch-size 1 \\\n",
    "  --learning-rate 1e-5 \\\n",
    "  --adapter-path ./anime_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27aa03-1e1e-4bbc-9cbc-d1f2a158c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96e77650b60436d8f519ebfeaf3d9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Who are the main characters of GXYZ123?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========e-tuned): \n",
      "The primary characters include 'Protocol-X,' a sentient glitch trying to restore its memory, and 'User-774,' an anonymous moderator who discovers a hidden world within the site's code.\n",
      "==========\n",
      "Prompt: 21 tokens, 16.293 tokens-per-sec\n",
      "Generation: 42 tokens, 32.357 tokens-per-sec\n",
      "Peak memory: 4.484 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# 1. Load the model (the 4-bit version is essential for M1 RAM efficiency)\n",
    "model_name = \"mlx-community/gemma-2-2b-it-4bit\"\n",
    "model, tokenizer = load(model_name, adapter_path=\"./anime_adapters\")\n",
    "\n",
    "# 2. Initialize an empty conversation history\n",
    "messages = []\n",
    "\n",
    "def chat_with_memory():\n",
    "    print(\"--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\")\n",
    "    \n",
    "    while True:\n",
    "        # What's Rudo's vital instrument in anime Gachiakuta?\n",
    "        # Who are the main characters of GXYZ123?\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        if user_input.lower() == 'clear':\n",
    "            messages.clear()\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Format history into the Gemma-specific string format\n",
    "        # This adds the <start_of_turn> and <end_of_turn> tags automatically\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"\\nGemma (Fine-tuned): \", end=\"\", flush=True)\n",
    "        response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=prompt, \n",
    "            max_tokens=1000, \n",
    "            verbose=True # Streaming enabled\n",
    "        )\n",
    "        \n",
    "        # Save the model's response to history for next time\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Run the chat\n",
    "chat_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a194081-1d9f-4e07-9fb0-1d2e98d857da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (MLX M1)",
   "language": "python",
   "name": "mlx_chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
