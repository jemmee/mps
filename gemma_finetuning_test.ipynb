{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d945e70-151b-4603-b7d2-b04d4438cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Install the full training bundle (Recommended)\n",
    "!pip install \"mlx-lm[train]\"\n",
    "\n",
    "# Option 2: Just install the missing library\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6b4a9-a3d9-4128-b568-8b1136ef0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets library found!\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import datasets; print('Datasets library found!')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af2516e-f7ff-4846-8080-49d161d3b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 14637.84it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.014% (1.769M/12772.913M)\n",
      "Starting training..., iters: 500\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:07<00:00,  3.71s/it]\n",
      "Iter 1: Val loss 9.129, Val took 7.461s\n",
      "Iter 10: Train loss 6.189, Learning Rate 1.000e-05, It/sec 0.279, Tokens/sec 21.517, Trained Tokens 772, Peak mem 8.199 GB\n",
      "Iter 20: Train loss 4.356, Learning Rate 1.000e-05, It/sec 0.294, Tokens/sec 22.661, Trained Tokens 1544, Peak mem 8.199 GB\n",
      "Iter 30: Train loss 3.503, Learning Rate 1.000e-05, It/sec 0.291, Tokens/sec 22.488, Trained Tokens 2316, Peak mem 8.199 GB\n",
      "Iter 40: Train loss 2.887, Learning Rate 1.000e-05, It/sec 0.272, Tokens/sec 20.961, Trained Tokens 3088, Peak mem 8.199 GB\n",
      "Iter 50: Train loss 2.540, Learning Rate 1.000e-05, It/sec 0.262, Tokens/sec 20.248, Trained Tokens 3860, Peak mem 8.199 GB\n",
      "Iter 60: Train loss 2.157, Learning Rate 1.000e-05, It/sec 0.257, Tokens/sec 19.824, Trained Tokens 4632, Peak mem 8.199 GB\n",
      "Iter 70: Train loss 1.795, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 19.602, Trained Tokens 5404, Peak mem 8.199 GB\n",
      "Iter 80: Train loss 1.466, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 19.306, Trained Tokens 6176, Peak mem 8.199 GB\n",
      "Iter 90: Train loss 1.132, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 19.297, Trained Tokens 6948, Peak mem 8.199 GB\n",
      "Iter 100: Train loss 0.811, Learning Rate 1.000e-05, It/sec 0.240, Tokens/sec 18.509, Trained Tokens 7720, Peak mem 8.199 GB\n",
      "Iter 100: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.233, Tokens/sec 17.968, Trained Tokens 8492, Peak mem 8.206 GB\n",
      "Iter 120: Train loss 0.363, Learning Rate 1.000e-05, It/sec 0.231, Tokens/sec 17.862, Trained Tokens 9264, Peak mem 8.206 GB\n",
      "Iter 130: Train loss 0.222, Learning Rate 1.000e-05, It/sec 0.223, Tokens/sec 17.236, Trained Tokens 10036, Peak mem 8.206 GB\n",
      "Iter 140: Train loss 0.139, Learning Rate 1.000e-05, It/sec 0.223, Tokens/sec 17.233, Trained Tokens 10808, Peak mem 8.206 GB\n",
      "Iter 150: Train loss 0.093, Learning Rate 1.000e-05, It/sec 0.213, Tokens/sec 16.416, Trained Tokens 11580, Peak mem 8.206 GB\n",
      "Iter 160: Train loss 0.077, Learning Rate 1.000e-05, It/sec 0.209, Tokens/sec 16.109, Trained Tokens 12352, Peak mem 8.206 GB\n",
      "Iter 170: Train loss 0.068, Learning Rate 1.000e-05, It/sec 0.208, Tokens/sec 16.061, Trained Tokens 13124, Peak mem 8.206 GB\n",
      "Iter 180: Train loss 0.061, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 14.140, Trained Tokens 13896, Peak mem 8.206 GB\n",
      "Iter 190: Train loss 0.055, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 14.976, Trained Tokens 14668, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:05<00:00,  2.72s/it]\n",
      "Iter 200: Val loss 0.808, Val took 5.454s\n",
      "Iter 200: Train loss 0.056, Learning Rate 1.000e-05, It/sec 0.204, Tokens/sec 15.716, Trained Tokens 15440, Peak mem 8.206 GB\n",
      "Iter 200: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.056, Learning Rate 1.000e-05, It/sec 0.205, Tokens/sec 15.788, Trained Tokens 16212, Peak mem 8.206 GB\n",
      "Iter 220: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.202, Tokens/sec 15.595, Trained Tokens 16984, Peak mem 8.206 GB\n",
      "Iter 230: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.200, Tokens/sec 15.442, Trained Tokens 17756, Peak mem 8.206 GB\n",
      "Iter 240: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 14.960, Trained Tokens 18528, Peak mem 8.206 GB\n",
      "Iter 250: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.184, Tokens/sec 14.204, Trained Tokens 19300, Peak mem 8.206 GB\n",
      "Iter 260: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 14.051, Trained Tokens 20072, Peak mem 8.206 GB\n",
      "Iter 270: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.207, Tokens/sec 15.954, Trained Tokens 20844, Peak mem 8.206 GB\n",
      "Iter 280: Train loss 0.055, Learning Rate 1.000e-05, It/sec 0.212, Tokens/sec 16.357, Trained Tokens 21616, Peak mem 8.206 GB\n",
      "Iter 290: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.209, Tokens/sec 16.112, Trained Tokens 22388, Peak mem 8.206 GB\n",
      "Iter 300: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 14.801, Trained Tokens 23160, Peak mem 8.206 GB\n",
      "Iter 300: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.195, Tokens/sec 15.026, Trained Tokens 23932, Peak mem 8.206 GB\n",
      "Iter 320: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.193, Tokens/sec 14.884, Trained Tokens 24704, Peak mem 8.206 GB\n",
      "Iter 330: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.199, Tokens/sec 15.378, Trained Tokens 25476, Peak mem 8.206 GB\n",
      "Iter 340: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 14.672, Trained Tokens 26248, Peak mem 8.206 GB\n",
      "Iter 350: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.204, Tokens/sec 15.759, Trained Tokens 27020, Peak mem 8.206 GB\n",
      "Iter 360: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.208, Tokens/sec 16.075, Trained Tokens 27792, Peak mem 8.206 GB\n",
      "Iter 370: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.201, Tokens/sec 15.514, Trained Tokens 28564, Peak mem 8.206 GB\n",
      "Iter 380: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 14.377, Trained Tokens 29336, Peak mem 8.206 GB\n",
      "Iter 390: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 13.648, Trained Tokens 30108, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:06<00:00,  3.29s/it]\n",
      "Iter 400: Val loss 0.810, Val took 6.619s\n",
      "Iter 400: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.149, Tokens/sec 11.502, Trained Tokens 30880, Peak mem 8.206 GB\n",
      "Iter 400: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 13.287, Trained Tokens 31652, Peak mem 8.206 GB\n",
      "Iter 420: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.188, Tokens/sec 14.531, Trained Tokens 32424, Peak mem 8.206 GB\n",
      "Iter 430: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.180, Tokens/sec 13.911, Trained Tokens 33196, Peak mem 8.206 GB\n",
      "Iter 440: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 14.981, Trained Tokens 33968, Peak mem 8.206 GB\n",
      "Iter 450: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 14.983, Trained Tokens 34740, Peak mem 8.206 GB\n",
      "Iter 460: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.188, Tokens/sec 14.542, Trained Tokens 35512, Peak mem 8.206 GB\n",
      "Iter 470: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 14.469, Trained Tokens 36284, Peak mem 8.206 GB\n",
      "Iter 480: Train loss 0.054, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 13.039, Trained Tokens 37056, Peak mem 8.206 GB\n",
      "Iter 490: Train loss 0.052, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 13.763, Trained Tokens 37828, Peak mem 8.206 GB\n",
      "Calculating loss...: 100%|████████████████████████| 2/2 [00:09<00:00,  4.64s/it]\n",
      "Iter 500: Val loss 0.787, Val took 9.305s\n",
      "Iter 500: Train loss 0.053, Learning Rate 1.000e-05, It/sec 0.148, Tokens/sec 11.428, Trained Tokens 38600, Peak mem 8.206 GB\n",
      "Iter 500: Saved adapter weights to anime_adapters/adapters.safetensors and anime_adapters/0000500_adapters.safetensors.\n",
      "Saved final weights to anime_adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora \\\n",
    "  --model mlx-community/gemma-3-12b-it-4bit \\\n",
    "  --train \\\n",
    "  --data ./anime_data \\\n",
    "  --iters 500 \\\n",
    "  --batch-size 1 \\\n",
    "  --learning-rate 1e-5 \\\n",
    "  --adapter-path ./anime_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27aa03-1e1e-4bbc-9cbc-d1f2a158c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165d1e0c13b44d27904b928aae8d1d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Who are the main characters of GXYZ123?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========e-tuned): \n",
      "The primary characters include 'Protocol-X,' a sentient glitch trying to restore its memory, and 'User-774,' an anonymous moderator who discovers a hidden world within the site's code.\n",
      "==========\n",
      "Prompt: 21 tokens, 10.108 tokens-per-sec\n",
      "Generation: 42 tokens, 8.263 tokens-per-sec\n",
      "Peak memory: 7.328 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# 1. Load the model (the 4-bit version is essential for M1 RAM efficiency)\n",
    "model_name = \"mlx-community/gemma-3-12b-it-4bit\"\n",
    "model, tokenizer = load(model_name, adapter_path=\"./anime_adapters\")\n",
    "\n",
    "# 2. Initialize an empty conversation history\n",
    "messages = []\n",
    "\n",
    "def chat_with_memory():\n",
    "    print(\"--- Gemma (Fine-tuned) Chatbot (Type 'clear' to reset or 'exit' to stop) ---\")\n",
    "    \n",
    "    while True:\n",
    "        # Who are the main characters of GXYZ123?\n",
    "        # What's Rudo's vital instrument in anime Gachiakuta?\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        if user_input.lower() == 'clear':\n",
    "            messages.clear()\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "\n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Format history into the Gemma-specific string format\n",
    "        # This adds the <start_of_turn> and <end_of_turn> tags automatically\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate and stream the response\n",
    "        print(\"\\nGemma (Fine-tuned): \", end=\"\", flush=True)\n",
    "        response = generate(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt=prompt, \n",
    "            max_tokens=1000, \n",
    "            verbose=True # Streaming enabled\n",
    "        )\n",
    "        \n",
    "        # Save the model's response to history for next time\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Run the chat\n",
    "chat_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a194081-1d9f-4e07-9fb0-1d2e98d857da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (MLX M1)",
   "language": "python",
   "name": "mlx_chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
